{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ph√¢n t√≠ch c·∫£m x√∫c v·ªõi LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment n√†y, ch√∫ng ta s·∫Ω d√πng m·∫°ng LSTM ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n ph√¢n t√≠ch c·∫£m x√∫c (Sentiment Analysis) tr√™n t·∫≠p d·ªØ li·ªáu vƒÉn b·∫£n. N·∫øu nh√¨n theo ki·ªÉu black box, ƒë·∫ßu v√†o c·ªßa b√†i to√°n l√† m·ªôt c√¢u ho·∫∑c ƒëo·∫°n vƒÉn b·∫£n v√† ƒë·∫ßu ra l√† tr·∫°ng th√°i t√≠ch c·ª±c, ti√™u c·ª±c hay trung ho√† (positive - negative - neutral). Trong ph·∫°m vi c·ªßa assignment n√†y, ch√∫ng ta ch·ªâ quan t√¢m ƒë·∫øn hai tr·∫°ng th√°i c·∫£m x√∫c l√† positive v√† negative.\n",
    "\n",
    "![caption](Images/input_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G√≥c nh√¨n Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N·∫øu nh∆∞ ch√∫ng ta gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng ƒë·∫ßu v√†o l√† chu·ªói k√Ω t·ª± th√¨ r·∫•t kh√≥ ƒë·ªÉ th·ª±c hi·ªán c√°c thao t√°c bi·∫øn ƒë·ªïi nh∆∞ t√≠ch v√¥ h∆∞·ªõng (dot product) ho·∫∑c c√°c thu·∫≠t to√°n tr√™n m·∫°ng neural network nh∆∞ backpropagation. Thay v√¨ d·ªØ li·ªáu ƒë·∫ßu v√†o l√† m·ªôt chu·ªói, ch√∫ng ta c·∫ßn chuy·ªÉn ƒë·ªïi c√°c t·ª´ trong t·∫≠p t·ª´ ƒëi·ªÉn sang d·∫°ng vector s·ªë h·ªçc trong ƒë√≥ c√≥ th·ªÉ th·ª±c hi·ªán ƒë∆∞·ª£c c√°c ph√©p to√°n n√™u tr√™n.\n",
    "\n",
    "![caption](Images/word2vec.png)\n",
    "\n",
    "Trong h√¨nh minh ho·∫° ·ªü tr√™n, ta c√≥ th·ªÉ h√¨nh dung d·ªØ li·ªáu ƒë·∫ßu v√†o c·ªßa thu·∫≠t to√°n ph√¢n t√≠ch c·∫£m x√∫c l√† m·ªôt ma tr·∫≠n 16 x D chi·ªÅu. Trong ƒë√≥ 16 l√† s·ªë l∆∞·ª£ng t·ª´ trong c√¢u v√† D l√† s·ªë chi·ªÅu c·ªßa kh√¥ng gian vector ƒë·ªÉ bi·ªÉu di·ªÖn t·ª´. ƒê·ªÉ √°nh x·∫° t·ª´ m·ªôt t·ª´ sang m·ªôt vector, ch√∫ng ta s·ª≠ d·ª•ng ma tr·∫≠n word embedding nh∆∞ ƒë√£ th·ª±c hi·ªán trong b√†i Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T·∫≠p d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment n√†y, ch√∫ng t√¥i s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu review tr√™n trang Foody v·ªõi kho·∫£ng 30,000 m·∫´u ƒë∆∞·ª£c g√°n nh√£n. Trong ƒë√≥ c√≥ 15,000 m·∫´u positive v√† 15,000 m·∫´u negative. Ngu·ªìn: https://streetcodevn.com/blog/dataset. T·∫≠p d·ªØ li·ªáu n√†y ƒë√£ ƒë∆∞·ª£c ƒë√≠nh k√®m trong th∆∞ m·ª•c c·ªßa assignment 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√°c b∆∞·ªõc ƒë·ªÉ hu·∫•n luy·ªán tr√™n m·∫°ng RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C√≥ 5 b∆∞·ªõc ch√≠nh ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n ph√¢n t√≠ch c·∫£m x√∫c trong vƒÉn b·∫£n:\n",
    "\n",
    "    1) Hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh ph√°t sinh ra vector t·ª´ (nh∆∞ m√¥ h√¨nh Word2Vec) ho·∫∑c t·∫£i l√™n c√°c vector t·ª´ ti·ªÅn hu·∫•n luy·ªán.\n",
    "    2) T·∫°o ma tr·∫≠n ID cho t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán\n",
    "    3) T·∫°o m√¥ h√¨nh RNN v·ªõi c√°c ƒë∆°n v·ªã LSTM, s·ª≠ d·ª•ng tensorflow\n",
    "    4) Hu·∫•n luy·ªán m√¥ h√¨nh RNN v·ªõi d·ªØ li·ªáu ma tr·∫≠n ƒë√£ t·∫°o ·ªü b∆∞·ªõc 2\n",
    "    5) ƒê√°nh gi√° m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán v·ªõi t·∫≠p test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load t·∫≠p t·ª´ v·ª±ng v√† ma tr·∫≠n word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·∫ßu ti√™n, ƒë·ªÉ c√≥ th·ªÉ bi·∫øn ƒë·ªïi m·ªôt t·ª´ th√†nh m·ªôt vector, ch√∫ng ta s·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc ƒë√≥ (pretrained model). M√¥ h√¨nh ƒë√£ train tr∆∞·ªõc ƒë√≥ cho ti·∫øng Vi·ªát ƒë∆∞·ª£c l·∫•y ·ªü ƒë√¢y: https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.vi.300.vec.gz\n",
    "\n",
    "Tuy nhi√™n, s·ªë l∆∞·ª£ng t·ª´ v·ª±ng ti·∫øng Vi·ªát ƒë∆∞·ª£c hu·∫•n luy·ªán r·∫•t l·ªõn, kho·∫£ng 2M t·ª´. M·ªói t·ª´ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng m·ªôt vector 300 chi·ªÅu. V·ªõi k√≠ch th∆∞·ªõc g·ªëc c·ªßa ma tr·∫≠n word embedding nh∆∞ v·∫≠y s·∫Ω g√¢y kh√≥ khƒÉn cho vi·ªác load d·ªØ li·ªáu c≈©ng nh∆∞ ƒë∆∞a v√†o th∆∞ vi·ªán tensorflow ƒë·ªÉ hu·∫•n luy·ªán n√™n ch√∫ng t√¥i ƒë√£ t·ªëi gi·∫£n l·∫°i v·ªõi s·ªë l∆∞·ª£ng t·ª´ t·ªëi thi·ªÉu ƒë·ªÉ c√≥ th·ªÉ ch·∫°y ƒë∆∞·ª£c tr√™n t·∫≠p d·ªØ li·ªáu review v·ªÅ ƒë·ªì ƒÉn c·ªßa Foody.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# L∆ØU √ù: C·∫¶N PH·∫¢I CH·ªàNH L·∫†I ƒê∆Ø·ªúNG D·∫™N N√ÄY TH√ÄNH TH∆Ø M·ª§C CH·ª®A C√ÅC FILE ASSIGNMENT3\n",
    "# CH·ªÆ 'drive' c√≥ nghƒ©a l√† th∆∞ m·ª•c m·∫∑c ƒë·ªãnh c·ªßa Google drive\n",
    "currentDir = '.'\n",
    "\n",
    "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "print('Simplified vocabulary loaded!')\n",
    "wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ ch·∫Øc ch·∫Øn m·ªçi d·ªØ li·ªáu ƒë∆∞·ª£c load l√™n m·ªôt c√°ch ch√≠nh x√°c, ch√∫ng ta c·∫ßn ki·ªÉm tra xem s·ªë l∆∞·ª£ng t·ª´ trong t·ª´ ƒëi·ªÉn r√∫t g·ªçn v√† s·ªë chi·ªÅu c·ªßa ma tr·∫≠n word embedding c√≥ kh·ªõp v·ªõi nhau hay kh√¥ng? Trong tr∆∞·ªùng h·ª£p n√†y s·ªë t·ª´ m√† ch√∫ng t√¥i gi·ªØ l·∫°i l√† 19,899 v√† s·ªë chi·ªÅu trong kh√¥ng gian bi·ªÉu di·ªÖn l√† 300 chi·ªÅu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary:  19899\n",
      "Size of the word embedding matrix:  (19899, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Size of the vocabulary: ', len(wordsList))\n",
    "print('Size of the word embedding matrix: ', wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec tr√™n m·ªôt t·ª´ ƒë∆°n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ c√≥ th·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c vector bi·ªÉu di·ªÖn c·ªßa m·ªôt t·ª´ ti·∫øng Vi·ªát. ƒê·∫ßu ti√™n ch√∫ng ta s·∫Ω x√°c ƒë·ªãnh xem v·ªã tr√≠ c·ªßa t·ª´ ƒë√≥ trong wordsList. Sau ƒë√≥ l·∫•y vector ·ªü d√≤ng t∆∞∆°ng ·ª©ng tr√™n tr√™n ma tr·∫≠n wordVectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of `ngon` in wordsList:  14598\n",
      "Vector representation of `ngon` is:  [-2.040e-02 -9.800e-03  2.290e-01 -3.770e-02  5.430e-02 -2.680e-02\n",
      "  2.190e-02 -6.290e-02 -2.200e-02 -1.010e-02  8.300e-03 -8.810e-02\n",
      " -3.630e-02  7.820e-02 -7.780e-02 -4.930e-02 -6.600e-03 -1.026e-01\n",
      " -1.040e-02  5.380e-02  4.100e-02  6.530e-02 -2.770e-02 -6.340e-02\n",
      "  2.270e-02  4.420e-02  3.340e-02 -4.960e-02  8.290e-02 -3.990e-02\n",
      "  3.750e-02  1.800e-02 -1.115e-01 -7.200e-02 -5.060e-02 -1.051e-01\n",
      " -4.560e-02 -1.765e-01 -3.300e-02 -6.800e-03  5.580e-02 -4.180e-02\n",
      "  4.380e-02  4.940e-02  7.400e-03  4.020e-02 -8.850e-02 -9.840e-02\n",
      " -5.210e-02 -5.500e-03  3.730e-02 -8.460e-02 -6.910e-02 -4.980e-02\n",
      " -3.910e-02 -4.980e-02 -8.690e-02  6.100e-03 -5.360e-02 -3.800e-03\n",
      "  1.162e-01 -4.160e-02  5.000e-03 -7.240e-02 -3.320e-02  1.800e-02\n",
      "  1.200e-02 -4.420e-02  1.350e-01  6.580e-02 -1.110e-02  1.960e-02\n",
      "  1.750e-02  2.010e-02  2.200e-03  1.810e-01 -6.610e-02 -6.860e-02\n",
      " -4.690e-02  7.890e-02  6.880e-02 -5.320e-02  2.770e-02  5.710e-02\n",
      " -1.183e-01  4.170e-02 -8.200e-02 -5.900e-02  8.790e-02  9.640e-02\n",
      "  6.000e-02  1.330e-02 -3.640e-02 -1.110e-02 -2.200e-02  1.770e-02\n",
      " -3.420e-02 -4.020e-02  3.590e-02  1.467e-01 -1.730e-02 -2.650e-02\n",
      "  6.400e-02  7.000e-03 -3.930e-02 -5.540e-02 -4.360e-02  8.000e-02\n",
      " -5.480e-02  3.840e-02 -8.330e-02  7.070e-02 -9.100e-03  2.480e-02\n",
      " -7.500e-03  3.030e-02 -6.600e-03 -9.800e-03  7.640e-02 -9.300e-03\n",
      "  2.330e-02 -2.000e-02  5.970e-02 -2.680e-02 -2.000e-02 -9.700e-03\n",
      " -5.310e-02 -9.820e-02 -2.570e-02  2.400e-02 -5.860e-02  1.820e-02\n",
      " -4.280e-02  9.580e-02  3.400e-02 -7.100e-03 -6.200e-03  1.239e-01\n",
      "  4.830e-02 -4.050e-02  4.810e-02 -1.093e-01  1.540e-02 -3.860e-02\n",
      "  1.250e-01 -7.950e-02  6.800e-03  7.420e-02  5.500e-02 -4.130e-02\n",
      " -2.090e-02 -2.250e-02  3.960e-02 -1.086e-01 -2.200e-02 -4.420e-02\n",
      "  1.965e-01 -2.260e-02  1.196e-01  1.200e-02  1.199e-01 -8.700e-03\n",
      "  4.260e-02  3.460e-02 -3.780e-02  1.951e-01 -9.300e-03 -6.260e-02\n",
      "  2.730e-02  7.340e-02  1.800e-03  5.080e-02 -3.470e-02 -9.680e-02\n",
      " -1.278e-01  3.790e-02  6.920e-02 -5.300e-02 -1.020e-01  1.076e-01\n",
      "  1.361e-01 -7.390e-02  9.650e-02 -2.250e-02  1.597e-01 -2.750e-02\n",
      " -4.200e-03  1.032e-01 -4.910e-02 -7.100e-03 -1.840e-02  7.240e-02\n",
      " -2.040e-02 -5.010e-02 -2.000e-04 -4.190e-02 -5.720e-02 -8.000e-03\n",
      "  9.780e-02 -7.130e-02 -6.070e-02 -1.740e-02 -1.290e-02  8.250e-02\n",
      "  6.600e-03 -2.250e-02 -5.100e-02  6.520e-02 -1.870e-02  5.790e-02\n",
      "  1.814e-01 -1.220e-01  4.770e-02  5.300e-02 -4.230e-02  2.139e-01\n",
      " -9.100e-03  1.314e-01 -3.600e-02 -3.780e-02  4.260e-02  3.000e-04\n",
      " -8.200e-02  1.570e-02 -1.380e-02  3.420e-02 -2.080e-02  1.790e-01\n",
      "  5.240e-02 -1.464e-01  6.330e-02  5.620e-02  2.000e-03 -6.490e-02\n",
      "  4.000e-04 -1.310e-02  1.020e-02  6.380e-02 -1.190e-02  2.440e-02\n",
      " -1.430e-02  1.027e-01  3.200e-03 -1.120e-02  8.270e-02  5.690e-02\n",
      "  2.740e-02 -9.800e-02 -3.150e-02 -9.750e-02 -1.660e-02  7.640e-02\n",
      " -4.960e-02 -7.940e-02  1.177e-01 -2.800e-03  6.860e-02 -5.930e-02\n",
      "  7.470e-02  5.790e-02  3.450e-02  5.550e-02 -3.380e-02  1.292e-01\n",
      "  3.840e-02  7.440e-02 -6.450e-02  2.470e-02 -1.810e-02  9.840e-02\n",
      " -1.329e-01 -6.380e-02 -8.360e-02 -3.580e-02  6.500e-03  8.240e-02\n",
      " -6.140e-02 -1.116e-01  2.310e-02  8.070e-02 -1.670e-02  4.150e-02\n",
      " -8.210e-02  6.290e-02 -5.580e-02  2.600e-03 -2.170e-02  3.200e-03\n",
      " -5.500e-03  6.040e-02  2.990e-02 -1.061e-01  5.200e-02  7.560e-02\n",
      "  6.250e-02  1.007e-01 -1.080e-01 -5.420e-02 -6.620e-02  6.080e-02]\n"
     ]
    }
   ],
   "source": [
    "ngon_idx = wordsList.index('ngon')\n",
    "print('Index of `ngon` in wordsList: ', ngon_idx)\n",
    "ngon_vec = wordVectors[ngon_idx]\n",
    "print('Vector representation of `ngon` is: ', ngon_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.1: Word2Vec ƒë·ªÉ bi·ªÉu di·ªÖn m·ªôt ƒëo·∫°n vƒÉn b·∫£n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N√¢ng c·∫•p h∆°n so v·ªõi phi√™n b·∫£n Word2Vec cho t·ª´ ƒë∆°n, ph·∫ßn n√†y ch√∫ng ta s·∫Ω bi·ªÉu di·ªÖn m·ªôt c√¢u d∆∞·ªõi d·∫°ng m·ªôt ma tr·∫≠n g·ªìm c√°c vector bi·ªÉu di·ªÖn c·ªßa t·ª´ng t·ª´ ch·ªìng l√™n nhau.\n",
    "\n",
    "V√≠ d·ª• nh∆∞ ch√∫ng ta mu·ªën bi·ªÉu di·ªÖn c√¢u \"M√≥n n√†y ƒÉn ho√†i kh√¥ng bi·∫øt ch√°n\". ƒê·∫ßu ti√™n, v·ªõi m·ªói t·ª´ trong c√¢u ta s·∫Ω t√¨m ch·ªâ s·ªë t∆∞∆°ng ·ª©ng trong t·ª´ ƒëi·ªÉn v√† l∆∞u v√†o vector ƒë·∫∑t t√™n l√† 'sentenceIndexes'. Sau ƒë√≥, ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng h√†m tra c·ª©u ma tr·∫≠n word embedding c·ªßa th∆∞ vi·ªán Tensorflow tf.nn.embedding_lookup ƒë·ªÉ tra c√°c vector t·∫°i c√°c ch·ªâ s·ªë trong 'sentenceIndexes'. Nh∆∞ v·∫≠y n·∫øu ch√∫ng ta s·ª≠ d·ª•ng t·ªëi ƒëa 10 t·ª´ ƒë·ªÉ l∆∞u tr·ªØ cho m·ªôt c√¢u th√¨ ma tr·∫≠n bi·ªÉu di·ªÖn cho c√¢u s·∫Ω l√† m·ªôt ma tr·∫≠n k√≠ch th∆∞·ªõc 10 x 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Row index for each word:  [  119  8136  4884 18791 16614 15951  3371     0     0     0]\n",
      "Sentence representation of word vectors:\n",
      "(10, 300)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "maxSeqLength = 10   #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "sentenceIndexes = np.zeros((maxSeqLength), dtype='int32')\n",
    "\n",
    "# TODO 3.1: G√°n ch·ªâ s·ªë c·ªßa c√°c t·ª´ trong c√¢u v√† 'sentenceIndexes'\n",
    "sentence = 'M√≥n n√†y ƒÉn ho√†i kh√¥ng bi·∫øt ch√°n'\n",
    "for i, word in enumerate(sentence.split()):\n",
    "    word = word.lower()\n",
    "    if word in wordsList:\n",
    "        sentenceIndexes[i] = wordsList.index(word)\n",
    "\n",
    "# C√°c ch·ªâ s·ªë 7, 8, 9 c·ªßa sentenceIndexes  v·∫´n ƒë∆∞·ª£c g√°n b·∫±ng 0 nh∆∞ c≈©\n",
    "print(sentenceIndexes.shape)\n",
    "print('Row index for each word: ', sentenceIndexes)\n",
    "\n",
    "# Ma tr·∫≠n bi·ªÉu di·ªÖn:\n",
    "print('Sentence representation of word vectors:')\n",
    "print(tf.nn.embedding_lookup(wordVectors,sentenceIndexes).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N·∫øu nh∆∞ th·ª±c hi·ªán ƒë√∫ng th√¨ vector 'sentenceIndexes' s·∫Ω c√≥ gi√° tr·ªã l√†: [119, 8136, 4884, 18791, 16614, 15951, 3371, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kh·∫£o s√°t t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán v√† t·∫°o ma tr·∫≠n ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment 3, ch√∫ng t√¥i s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu l·∫•y t·ª´ trang web Foody tr√™n mi·ªÅn d·ªØ li·ªáu li√™n quan ƒë·∫øn ·∫©m th·ª±c. T·∫≠p d·ªØ li·ªáu bao g√¥m 15.000 review t√≠ch c·ª±c ƒë·∫∑t trong th∆∞ m·ª•c 'positiveReviews' v√† 15.000 review ti√™u c·ª±c ƒë·∫∑t trong th∆∞ m·ª•c 'negativeReviews'. Do kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu l·ªõn, n·∫øu ch√∫ng ta ch·ªçn s·ªë l∆∞·ª£ng t·ª´ t·ªëi ƒëa (maxSeqLength) qu√° cao th√¨ s·∫Ω b·ªã l√£ng ph√≠ khi bi·ªÉu di·ªÖn ·ªü nh·ªØng c√¢u review qu√° ng·∫Øn. Ng∆∞·ª£c l·∫°i, n·∫øu s·ª≠ d·ª•ng s·ªë l∆∞·ª£ng t·ª´ t·ªëi ƒëa qu√° √≠t th√¨ s·∫Ω b·ªã b·ªè l·ª° nh·ªØng t·ª´ quan tr·ªçng gi√∫p cho vi·ªác ph√¢n t√≠ch c·∫£m x√∫c.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 30000\n",
      "The total number of words in the files is 1770824\n",
      "The average number of words in the files is 59.02746666666667\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng th∆∞ vi·ªán Matplot ƒë·ªÉ minh ho·∫° ph√¢n b·ªë v·ªÅ chi·ªÅu d√†i c·ªßa c√°c c√¢u review trong t·∫≠p d·ªØ li·ªáu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D·ª±a tr√™n bi·ªÉu ƒë·ªì histogram ·ªü tr√™n ch√∫ng ta c√≥ th·ªÉ th·∫•y l√† 180 l√† k·∫øt qu·∫£ t∆∞∆°ng ƒë·ªëi h·ª£p l√Ω. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ c√≥ c·∫£m nh·∫≠n r√µ h∆°n v·ªÅ d·ªØ li·ªáu, ch√∫ng ta c√≥ th·ªÉ hi·ªÉn th·ªã m·ªôt s·ªë review b·∫•t k·ª≥ nh∆∞ sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A positive sentence: \n",
      "Minh ƒë·∫øn ƒë√¢y l√† v√¨ nh√¢n_vi√™n ·ªü ƒë√¢y . L√∫c m√¨nh ƒë·∫øn th√¨ t·ªëi mu·ªôn r·ªìi m√† kh√°ch v·∫´n ƒë√¥ng ng∆∞·ªùi m√† to√†n m·∫•y b·∫°n n·ªØ ( ch·∫Øc gi·ªëng m√¨nh ) . M√¨nh g·ªçi tr√† matcha √≥c ch√≥ v√† c·ª±c_k√¨ h√†i_l√≤ng . V√¨ m√¨nh ƒë·∫øn l√∫c ƒë√¥ng kh√°ch n√™n c·∫£m_gi√°c h∆°i b√≠ nh∆∞ng kh√¥ng_gian ƒë·∫πp v√† nh√¢n_vi√™n th√¨ tr√™n c·∫£ tuy·ªát_v·ªùi . T·ª´ nh√¢n_vi√™n g·ª≠i xe ƒë·∫øn d·ªçn_d·∫πp hay pha_ch·∫ø ƒë·ªÅu th√¢n_thi·ªán , d·ªÖ_th∆∞∆°ng .\n",
      "\n",
      "A negative sentence: \n",
      "ƒêi·∫° ƒëi·ªÖm m√¨nh y√™u th√≠c v√† th∆∞·ªùng_xuy√™n gh√© . St·ªë ngon li b·ª± , c√≥ tr√°i_c√¢y ƒë·ªÖ l√™n tr√™n n√™n nh√¨n ƒë·∫πp v√† h·∫•p_d·∫´n , m√¨nh u·ªëng th∆∞·ªùng mix c√°c lo·∫°i vs nhau n√™n l√∫c_n√†o c≈©ng th·∫•y ngon üòô gi√°_c·∫£ c≈©ng ƒëc , c√≥ Wi-Fi nh∆∞ng h∆°i y·∫øu . Ch·ªâ gh√©t l√† gi·ªØ xe 10k , l·ªÖ l·ªôc l√† 20k üò´ mua v·ªÅ th√¨ ch·ªó_ƒë·ª©ng h∆°i h·∫πp , ng·ªìi u·ªëng c≈©ng v h∆°i ch·∫≠t do ng·ªìi s√°t 2b√™n h·∫Ωm ƒë·ªÖ ch·ª´a ƒëg cho xe v√† ng ra_v√†o . N·∫±m trong khu B√πi vi·ªán l√™n l√∫c_n√†o c≈©ng t·∫•p_n·∫≠p ƒë√¥ng vui üëç\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('A positive sentence: ')\n",
    "fname = positiveFiles[3] # Randomly select a positive file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "\n",
    "print('A negative sentence: ')\n",
    "fname = negativeFiles[10] # Randomly select a negative file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chu·∫©n ho√° vƒÉn b·∫£n v√† t√°ch t·ª´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ ti·∫øt ki·ªám c√¥ng s·ª©c v√† c≈©ng n·∫±m ngo√†i ph·∫°m vi c·ªßa kho√° h·ªçc, ch√∫ng t√¥i ƒë√£ chu·∫©n b·ªã s·∫µn t·∫≠p d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t√°ch t·ª´. Gi·ªØa hai t·ª´ c√≥ th·ªÉ gh√©p l·∫°i ƒë·ªÉ t·∫°o th√†nh m·ªôt kh√°i ni·ªám m·ªõi ch√∫ng t√¥i s·ª≠ d·ª•ng k√Ω t·ª± '_' ƒë·ªÉ n·ªëi c√°c t·ª´ ƒë√≥. V√≠ d·ª•: 'sinh_vi√™n', 'sinh_h·ªçc'.\n",
    "\n",
    "Ch√∫ng t√¥i chu·∫©n b·ªã s·∫µn c√°c h√†m chu·∫©n ho√° vƒÉn b·∫£n nh·∫±m lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát. Tham kh·∫£o ·ªü h√†m 'cleanSentences'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B√¢y gi·ªù ch√∫ng ta s·∫Ω bi·ªÉu di·ªÖn 30.000 review d∆∞·ªõi d·∫°ng c√°c ch·ªâ s·ªë c·ªßa c√°c t·ª´. T·∫≠p d·ªØ li·ªáu positive v√† negative s·∫Ω ƒë∆∞·ª£c t√≠nh h·ª£p l·∫°i th√†nh m·ªôt ma tr·∫≠n 30000x180. Trong ƒë√≥ 30000 l√† s·ªë l∆∞·ª£ng review v√† 180 l√† s·ªë l∆∞·ª£ng t·ª´ t·ªëi ƒëa cho m·ªôt c√¢u. Do b∆∞·ªõc chu·∫©n b·ªã n√†y t·ªën kh√° nhi·ªÅu t√†i nguy√™n t√≠nh to√°n n√™n sau khi t√≠nh to√°n xong, ch√∫ng ta s·∫Ω l∆∞u l·∫°i ƒë·ªÉ s·ª≠ d·ª•ng cho nh·ªØng l·∫ßn ch·∫°y th√≠ nghi·ªám sau. Ma tr·∫≠n l∆∞u tr·ªØ c√°c ch·ªâ s·ªë n√†y l√†: 'ids'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.2: x√°c ƒë·ªãnh ch·ªâ s·ªë c·ªßa t·ª´ng t·ª´ trong review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ph·∫ßn n√†y ch√∫ng ta s·∫Ω ti·∫øn h√†nh tra c·ª©u t·ª´ng t·ª´ trong review, sau ƒë√≥ g√°n v√†o ma tr·∫≠n 'ids'. Trong ƒë√≥ ch·ªâ s·ªë d√≤ng c·ªßa ma tr·∫≠n t∆∞∆°ng ·ª©ng v·ªõi file review, ch·ªâ s·ªë c·ªôt c·ªßa ma tr·∫≠n t∆∞∆°ng ·ª©ng v·ªõi m·ªôt t·ª´ c·ªßa review. Tr∆∞·ªùng h·ª£p t·ª´ n√†o kh√¥ng c√≥ trong t·∫≠p t·ª´ ƒëi·ªÉn th√¨ ta s·∫Ω g√°n b·∫±ng ch·ªâ s·ªë c·ªßa t·ª´ 'UNK' (unknow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positiveReviews/27034.txt',\n",
       " 'positiveReviews/30572.txt',\n",
       " 'positiveReviews/414.txt',\n",
       " 'positiveReviews/3350.txt',\n",
       " 'positiveReviews/4288.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positiveFiles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files are indexed!\n",
      "Negative files are indexed!\n"
     ]
    }
   ],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "nFiles = 0\n",
    "# Index of Unknow word\n",
    "unk_idx = wordsList.index('UNK')\n",
    "\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
    "        nIndexes = 0\n",
    "        lines=f.readlines()\n",
    "        flag_max = False\n",
    "        for line in lines:\n",
    "            cleanedLine = cleanSentences(line)\n",
    "            split = cleanedLine.split()\n",
    "        \n",
    "            for word in split:\n",
    "                # TODO 3.2: N·∫øu 'word' thu·ªôc t·∫≠p 'wordsList' th√¨ g√°n ch·ªâ s·ªë c·ªßa 'word' v√†o ma tr·∫≠n ids\n",
    "                if word in wordsList:\n",
    "                    ids[nFiles][nIndexes] = wordsList.index(word)\n",
    "                else:\n",
    "                    ids[nFiles][nIndexes] = unk_idx\n",
    "                # Ng∆∞·ª£c l·∫°i: g√°n 'unk_idx' v√†o ma tr·∫≠n ids\n",
    "            \n",
    "                nIndexes = nIndexes + 1\n",
    "                if nIndexes >= maxSeqLength:\n",
    "                    flag_max = True\n",
    "                    break\n",
    "            if flag_max:\n",
    "                break\n",
    "        nFiles = nFiles + 1 \n",
    "\n",
    "print('Positive files are indexed!')\n",
    "for pf in negativeFiles:\n",
    "    with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
    "        nIndexes = 0\n",
    "        lines=f.readlines()\n",
    "        flag_max = False\n",
    "        for line in lines:\n",
    "            cleanedLine = cleanSentences(line)\n",
    "            split = cleanedLine.split()\n",
    "        \n",
    "            for word in split:\n",
    "                # TODO 3.2: N·∫øu 'word' thu·ªôc t·∫≠p 'wordsList' th√¨ g√°n ch·ªâ s·ªë c·ªßa 'word' v√†o ma tr·∫≠n ids\n",
    "                if word in wordsList:\n",
    "                    ids[nFiles][nIndexes] = wordsList.index(word)\n",
    "                else:\n",
    "                    ids[nFiles][nIndexes] = unk_idx\n",
    "                # Ng∆∞·ª£c l·∫°i: g√°n 'unk_idx' v√†o ma tr·∫≠n ids\n",
    "            \n",
    "                nIndexes = nIndexes + 1\n",
    "                if nIndexes >= maxSeqLength:\n",
    "                    flag_max = True\n",
    "                    break\n",
    "            if flag_max:\n",
    "                break\n",
    "        nFiles = nFiles + 1\n",
    "\n",
    "print('Negative files are indexed!')\n",
    "# Save ids Matrix for future uses.\n",
    "np.save(os.path.join(currentDir,'idsMatrix.npy'), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indexes of the first review:  [11975  8136 17108 13863  5596 14855  5596   799 10774  4884 11975 14753\n",
      " 14794  1825 10774 12615 14855  4711  8031   880  2616  8262 10630  4558\n",
      "  5446  2465  3364  4639  6708  6521 11440 17292  4135  4957 17330 12145\n",
      " 18393 15387 16760 14017 16521  6244 10231  7692 11440 14341   547 14475\n",
      "  2223  7087 18393  7424 18655  9673 11068 14595 10139 11440 19646 10453\n",
      " 11975  4558 15444 18407 19363 16624  3364 14017 16521  8884  1906 17821\n",
      " 15522   547  1868 12145 18503  1906  6512  9704  2997 11068 14598 15570\n",
      "  5596 13258  4884 16995  6512  9704  6874 12990 11047 10346 17162  6915\n",
      " 18584  4884 17130  2611 18503 16995  4711 12990 12145  1047  1731 13315\n",
      "  8880 14017 16521 16807 19261 16346  6512  9704  6512  9704 17572  6874\n",
      "  8231  1368 19193  9673  4682  8231  1238  4558   294  5596 11440 16812\n",
      "  1450  2451  1807   495  1346  1807 16995  6512  9704  8548  5596 13255\n",
      "  7836 15605  5767 17636  1868 11966  5013  4236 18655   119  4884 10009\n",
      " 15341  1074  2789 17821 14598 18503   547 14017 16521  5596  5074 14341\n",
      "  2345  8591 15213  3167  5767  8591 15213 12413 15213  4964  6365 14855]\n"
     ]
    }
   ],
   "source": [
    "# L∆ØU √ù: B∆∞·ªõc th·ª±c hi·ªán tr√™n t∆∞∆°ng ƒë·ªëi m·∫•t th·ªùi gian.\n",
    "# Tr∆∞·ªùng h·ª£p ƒë√£ t√≠nh to√°n v√† l∆∞u ma tr·∫≠n 'ids' r·ªìi th√¨ ta c√≥ th·ªÉ load l√™n ƒë·ªÉ s·ª≠ d·ª•ng lu√¥n\n",
    "ids = np.load(os.path.join(currentDir,'idsMatrix.npy'))\n",
    "print('Word indexes of the first review: ', ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qu√°n n√†y g·∫ßn nh√† m√¨nh n√™n m√¨nh hay ƒë·∫øn ƒÉn qu√°n m·ªü_c·ª≠a t·ª´ s√°ng ƒë·∫øn t·ªëi n√™n hk s·ª£ canh gi·ªù_gi·∫•c kh√¥ng_gian tho√°ng c√≥ m√°y_l·∫°nh m√°t l·∫Øm lun kh√° y√™n_tƒ©nh ƒëi ƒë√¥ng n√≥i_chuy·ªán ·ªìn qu√° th√¨ h∆°i k√¨ v√¨ ·ªü ƒë√¢y ƒëa_s·ªë ng ta ƒëi u·ªëng cafe l√†m_vi·ªác h·ªçc b√†i h∆°i y√™n_l·∫∑ng trang_tr√≠ l·ªãch_s·ª± c·ª±c s·∫°ch_s·∫Ω n·∫øu ƒëi chi·ªÅu chi·ªÅu_t·ªëi qu√°n c√≥ m·ªü nh·∫°c_nh·∫π nghe ph√™ l·∫Øm ·ªü ƒë√¢y ngo√†i b√°n kem v√† cafe ra th√¨ c√≤n b√°n m·ª≥ √Ω n·ªØa c·ª±c ngon nh√© m√¨nh th∆∞·ªùng ƒÉn ph·∫ßn m·ª≥ √Ω b√≤ ph√¥_mai d√†nh cho c√°c b·∫°n th√≠ch ƒÉn b√©o_b√©o 42k c√≤n ph·∫ßn hk ph√¥_mai th√¨ r·∫ª h∆°n gi√°_c·∫£ ph√π_h·ª£p ·ªü ƒë√¢y menu ƒë·ªß ki·ªÉu m·ª≥ √Ω m·ª≥ √Ω g√† b√≤ ph·ª•c_v·ª• t·ªët th√¢n_thi·ªán l·ªãch_s·ª± th·ªùi_gian ph·ª•c_v·ª• nhanh c√≥ k·ª≥ m√¨nh ƒëi zs b·∫°n_b√® t·ªïng_c·ªông 6 ƒë·ª©a k√™u 6 ph·∫ßn m·ª≥ √Ω t·ª•i m√¨nh ng·ªìi nc t√≠ l√† ƒëem ra li·ªÅn √† c√°ch trang_tr√≠ m√≥n ƒÉn b·∫Øt_m·∫Øt tr√† ƒë√° free kem ngon c√≤n cafe ·ªü ƒë√¢y m√¨nh ch∆∞a u·ªëng v·ªã_tr√≠ trong h·∫ªm n√≥i l√† trong h·∫ªm ch·ª© h·∫ªm c≈©ng b·ª± n√™n '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''\n",
    "for w in ids[0]:\n",
    "  s += wordsList[w]\n",
    "  s += ' '\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N·∫øu nh∆∞ qu√° tr√¨nh chuy·ªÉn t·ª´ c√¢u d·∫°ng vƒÉn b·∫£ng sang vector c√°c ch·ªâ s·ªë trong t·ª´ ƒëi·ªÉn ·ªü tr√™n ƒë√∫ng th√¨ ids[0] s·∫Ω nh·∫≠n gi√° tr·ªã: [19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821 ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X√¢y d·ª±ng h√†m l·∫•y d·ªØ li·ªáu train v√† test theo t·ª´ng batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D∆∞·ªõi ƒë√¢y ch√∫ng t√¥i x√¢y d·ª±ng c√°c h√†m ƒë·ªÉ l·∫•y d·ªØ li·ªáu train v√† test theo t·ª´ng batch. B·∫°n h√£y gi·∫£i th√≠ch t·∫°i sao l·∫°i c√≥ c√°c con s·ªë 13999, 14999, 15999, 29999 nh√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getFullData():\n",
    "    train_y = []\n",
    "    test_y = []\n",
    "    train_x = np.zeros([28000, maxSeqLength])\n",
    "    test_x = np.zeros([2000, maxSeqLength])\n",
    "    train_num = 0\n",
    "    test_num = 0\n",
    "    \n",
    "    for i in range(30000):    \n",
    "        if i < 14000:\n",
    "            train_y.append(1)\n",
    "            train_x[train_num] = ids[i:i+1]\n",
    "            train_num += 1\n",
    "            \n",
    "        elif i >= 14000 and i <15000:\n",
    "            test_y.append(1)\n",
    "            test_x[test_num] = ids[i:i+1]\n",
    "            test_num += 1\n",
    "            \n",
    "        elif i >= 15000 and i < 16000:\n",
    "            test_y.append(0)\n",
    "            test_x[test_num] = ids[i:i+1]\n",
    "            test_num += 1\n",
    "            \n",
    "        elif i >= 16000:\n",
    "            train_y.append(0)\n",
    "            train_x[train_num] = ids[i:i+1]\n",
    "            train_num += 1\n",
    "    return train_x, np.array(train_y), test_x, np.array(test_y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. X√¢y d·ª±ng RNN Model v·ªõi Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·∫ßu ti√™n ch√∫ng t√¥i s·∫Ω kh·ªüi t·∫°o c√°c tham s·ªë cho m√¥ h√¨nh m·∫°ng RNN v·ªõi c√°c cell l√† c√°c LSTM. Ki·∫øn tr√∫c m·∫°ng ·ªü ƒë√¢y bao g·ªìm 128 ƒë∆°n v·ªã cho m·ªói l·ªõp, s·ªë l∆∞·ª£ng layer l√† 2, s·ªë l∆∞·ª£ng ph√¢n l·ªõp l√† 2 v√† s·ªë v√≤ng l·∫∑p khi hu·∫•n luy·ªán l√† 30000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paramters\n",
    "numDimensions = 300\n",
    "batchSize = 64\n",
    "lstmUnits = 128\n",
    "nLayers = 2\n",
    "numClasses = 2\n",
    "iterations = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu input v√† ouput, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng hai ki·ªÉu d·ªØ li·ªáu placeholder. M·ªôt trong nh·ªØng ƒëi·ªÅu quan tr·ªçng nh·∫•t khi kh·ªüi t·∫°o c√°c bi·∫øn input v√† output n√†y l√† x√°c ƒë·ªãnh k√≠ch th∆∞·ªõc c·ªßa c√°c tensor. M·ªói output c·ªßa m·∫°ng (hay c√≤n g·ªçi l√† label) s·∫Ω l√† m·ªôt vector one hot v·ªõi hai gi√° tr·ªã t∆∞∆°ng ·ª©ng v·ªõi hai lo·∫°i c·∫£m x√∫c: [1, 0] cho positive v√† [0, 1] cho negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/data_batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.3: X√°c ƒë·ªãnh input v√† output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kh·ªüi t·∫°o hai bi·∫øn 'inputs' v√† 'labels' b·∫±ng ki·ªÉu placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 180)\n",
      "(28000,)\n",
      "(2000, 180)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "inputs, labels, test_inputs, test_labels = getFullData()\n",
    "\n",
    "train_index = np.arange(len(inputs))\n",
    "np.random.shuffle(train_index)\n",
    "inputs = inputs[train_index]\n",
    "labels = labels[train_index]\n",
    "inputs = tf.convert_to_tensor(inputs, dtype=tf.int32)\n",
    "labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "test_inputs = tf.convert_to_tensor(test_inputs, dtype=tf.int32)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "\n",
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "print(test_inputs.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau ƒë√≥ t·∫°o d·ªØ li·ªáu word vector t·ª´ kh·ªëi d·ªØ li·ªáu ƒë·∫ßu v√†o v·ªõi ma tr·∫≠n word embedding. N·∫øu nh∆∞ qu√° tr√¨nh kh·ªüi t·∫°o ƒë√∫ng th√¨ s·∫Ω t·∫°o ra c√°c ki·ªÉu d·ªØ li·ªáu sau:\n",
    "labels --> Tensor(\"Placeholder:0\", shape=(64, 2), dtype=float32)\n",
    "inputs --> Tensor(\"Placeholder_1:0\", shape=(64, 10), dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nh∆∞ v·∫≠y sau b∆∞·ªõc n√†y ch√∫ng ta ƒë√£ c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë∆∞a v√†o m·∫°ng m·∫°ng c√°c LSTM. ƒê·ªÉ kh·ªüi t·∫°o m·ªôt LSTM ch√∫ng ta s·ª≠ d·ª•ng h√†m tf.nn.rnn_cell.BasicLSTMCell. H√†m n√†y c·∫ßn tham s·ªë ƒë·∫ßu v√†o l√† s·ªë l∆∞·ª£ng ƒë∆°n v·ªã mu·ªën kh·ªüi t·∫°o. ƒê√¢y ch√≠nh l√† m·ªôt hyperparamter ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o tr∆∞·ªõc ƒë√≥.\n",
    "ƒê·ªÉ ch·ªëng l·∫°i vi·ªác overfitting, ch√∫ng ta s·ª≠ d·ª•ng l·ªõp dropout. \n",
    "\n",
    "ƒê·ªÉ tƒÉng t√≠nh ph·ª©c t·∫°p cho ki·∫øn tr√∫c m·∫°ng ch√∫ng ta ch·ªìng c√°c l·ªõp LSTM l√™n nhau (Stack LSTM Layers). Trong tr∆∞·ªùng h·ª£p n√†y ch√∫ng ta s·ª≠ d·ª•ng 2 l·ªõp LSTM. Vi·ªác ch·ªìng th√™m c√°c l·ªõp LSTM s·∫Ω gi√∫p cho m√¥ h√¨nh c√≥ kh·∫£ nƒÉng nh·ªõ nhi·ªÅu th√¥ng tin h∆°n nh∆∞ng ƒë·ªìng th·ªùi c≈©ng l√†m tƒÉng s·ªë l∆∞·ª£ng tham s·ªë khi hu·∫•n luy·ªán. ƒêi·ªÅu n√†y c≈©ng c√≥ nghƒ©a l√† s·∫Ω l√†m tƒÉng th·ªùi gian hu·∫•n luy·ªán c≈©ng nh∆∞ l√† c·∫ßn th√™m nhi·ªÅu d·ªØ li·ªáu h∆°n.\n",
    "\n",
    "Cu·ªëi c√πng l√† ƒë∆∞a to√†n b·ªô d·ªØ li·ªáu ƒë·∫ßu v√†o v√†o m·∫°ng c√°c LSTM s·ª≠ d·ª•ng h√†m tf.nn.dynamic_rnn. Chi ti·∫øt ki·∫øn tr√∫c m·∫°ng LSTM s·ª≠ d·ª•ng cho b√†i t·∫≠p n√†y ƒë∆∞·ª£c m√¥ t·∫£ trong h√¨nh sau:\n",
    "\n",
    "![caption](Images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi ra kh·ªèi m·∫°ng LSTM, bi·∫øn outputs s·∫Ω l√† m·ªôt tensor c√≥ k√≠ch th∆∞·ªõc [batchSize x maxSeqLength x lstmUnits], c·ª• th·ªÉ l√† [64 x 180 x 128]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau ƒë√≥, ch√∫ng ta ch·ªâ l·∫•y d·ªØ li·ªáu ·ªü LSTM cell cu·ªëi c√πng v√† cho ƒëi qua l·ªõp k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß ƒë·ªÉ ph√¢n lo·∫°i th√†nh 2 tr·∫°ng th√°i. Ch·ªâ s·ªë c·ªßa LSTM cell cu·ªëi c√πng l√† 179 (do c√≥ 180 cell theo chi·ªÅu ngang)  n√™n ƒë·ªÉ c√≥ th·ªÉ l·∫•y ƒë∆∞·ª£c gi√° tr·ªã ta s·∫Ω chuy·ªÉn v·ªã v·ªÅ tensor c√≥ k√≠ch th∆∞·ªõc [maxSeqLength x batchSize x lstmUnits] hay [180 x 64 x 128]. S·ª≠ d·ª•ng h√†m tf.gather ƒë·ªÉ l·∫•y tensor th·ª© 179 c√≥ k√≠ch th∆∞·ªõc [64 x 128] bao g·ªìm 64 m·∫´u vector 128 chi·ªÅu. Vector 128 chi·ªÅu n√†y s·∫Ω ƒë∆∞·ª£c ƒë∆∞a v√†o l·ªõp fully connected ƒë·ªÉ chuy·ªÉn ƒë·ªïi v·ªÅ vector 2 chi·ªÅu t∆∞∆°ng ·ª©ng v·ªõi 2 tr·∫°ng th√°i.\n",
    "\n",
    "L·ªõp k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß bao g·ªìm c√°c b·ªô tham s·ªë 'weight' v√† 'bias' ƒë·ªÉ th·ª±c hi·ªán vi·ªác d·ª± ƒëo√°n k·∫øt qu·∫£. B∆∞·ªõc n√†y ch√≠nh l√† t·∫°o m·ªôt l·ªõp Fully Connected nh∆∞ trong s∆° ƒë·ªì ki·∫øn tr√∫c m·∫°ng LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoLayerLSTM(tf.keras.Model):\n",
    "    def __init__(self, lstmUnits, wordVectors):\n",
    "        super(twoLayerLSTM, self).__init__()\n",
    "        self.wordVectors = wordVectors\n",
    "        self.lstmUnits = lstmUnits\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        x = tf.nn.embedding_lookup(self.wordVectors, input_tensor)\n",
    "        x = tf.keras.layers.LSTM(self.lstmUnits, return_sequences=True, dropout=0.2)(x)\n",
    "        x = tf.keras.layers.LSTM(self.lstmUnits, dropout=0.2)(x)\n",
    "        x = tf.keras.layers.Dense(32, activation='tanh')(x)\n",
    "        x = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "        return x\n",
    "\n",
    "    def model(self):\n",
    "        x = tf.keras.Input(shape=(180,), dtype=tf.int32)\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 180)]             0         \n",
      "_________________________________________________________________\n",
      "tf.compat.v1.nn.embedding_lo (None, 180, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 180, 16)           20288     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 22,977\n",
      "Trainable params: 22,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = twoLayerLSTM(lstmUnits, wordVectors).model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "%load_ext tensorboard\n",
    "\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq=5*28000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "45/45 [==============================] - 30s 612ms/step - loss: 0.6230 - accuracy: 0.7034 - val_loss: 0.5365 - val_accuracy: 0.7479\n",
      "Epoch 2/50\n",
      "45/45 [==============================] - 25s 561ms/step - loss: 0.5308 - accuracy: 0.7509 - val_loss: 0.4909 - val_accuracy: 0.7870\n",
      "Epoch 3/50\n",
      "45/45 [==============================] - 26s 575ms/step - loss: 0.4812 - accuracy: 0.7805 - val_loss: 0.4425 - val_accuracy: 0.8002\n",
      "Epoch 4/50\n",
      "45/45 [==============================] - 26s 576ms/step - loss: 0.4494 - accuracy: 0.7929 - val_loss: 0.4148 - val_accuracy: 0.8030\n",
      "Epoch 5/50\n",
      "45/45 [==============================] - 26s 573ms/step - loss: 0.4241 - accuracy: 0.8074 - val_loss: 0.3935 - val_accuracy: 0.8320\n",
      "Epoch 6/50\n",
      "45/45 [==============================] - 26s 577ms/step - loss: 0.4035 - accuracy: 0.8166 - val_loss: 0.3735 - val_accuracy: 0.8254\n",
      "Epoch 7/50\n",
      "45/45 [==============================] - 27s 598ms/step - loss: 0.3869 - accuracy: 0.8195 - val_loss: 0.3672 - val_accuracy: 0.8207\n",
      "Epoch 8/50\n",
      "45/45 [==============================] - 28s 616ms/step - loss: 0.3872 - accuracy: 0.8181 - val_loss: 0.3583 - val_accuracy: 0.8313\n",
      "Epoch 9/50\n",
      "45/45 [==============================] - 29s 637ms/step - loss: 0.3801 - accuracy: 0.8240 - val_loss: 0.3564 - val_accuracy: 0.8286\n",
      "Epoch 10/50\n",
      "45/45 [==============================] - 28s 618ms/step - loss: 0.3712 - accuracy: 0.8252 - val_loss: 0.3499 - val_accuracy: 0.8345\n",
      "Epoch 11/50\n",
      "45/45 [==============================] - 28s 633ms/step - loss: 0.3655 - accuracy: 0.8295 - val_loss: 0.3614 - val_accuracy: 0.8171\n",
      "Epoch 12/50\n",
      "45/45 [==============================] - 30s 661ms/step - loss: 0.3704 - accuracy: 0.8268 - val_loss: 0.3449 - val_accuracy: 0.8523\n",
      "Epoch 13/50\n",
      "45/45 [==============================] - 27s 607ms/step - loss: 0.3481 - accuracy: 0.8441 - val_loss: 0.3371 - val_accuracy: 0.8480\n",
      "Epoch 14/50\n",
      "45/45 [==============================] - 27s 602ms/step - loss: 0.3509 - accuracy: 0.8415 - val_loss: 0.3468 - val_accuracy: 0.8373\n",
      "Epoch 15/50\n",
      "45/45 [==============================] - 26s 579ms/step - loss: 0.3569 - accuracy: 0.8391 - val_loss: 0.3390 - val_accuracy: 0.8477\n",
      "Epoch 16/50\n",
      "45/45 [==============================] - 27s 605ms/step - loss: 0.3446 - accuracy: 0.8410 - val_loss: 0.3349 - val_accuracy: 0.8511\n",
      "Epoch 17/50\n",
      "45/45 [==============================] - 28s 633ms/step - loss: 0.3501 - accuracy: 0.8428 - val_loss: 0.3316 - val_accuracy: 0.8555\n",
      "Epoch 18/50\n",
      "45/45 [==============================] - 27s 613ms/step - loss: 0.3418 - accuracy: 0.8447 - val_loss: 0.3310 - val_accuracy: 0.8468\n",
      "Epoch 19/50\n",
      "45/45 [==============================] - 30s 671ms/step - loss: 0.3389 - accuracy: 0.8484 - val_loss: 0.3512 - val_accuracy: 0.8245\n",
      "Epoch 20/50\n",
      "45/45 [==============================] - 28s 628ms/step - loss: 0.3456 - accuracy: 0.8366 - val_loss: 0.3330 - val_accuracy: 0.8641\n",
      "Epoch 21/50\n",
      "45/45 [==============================] - 29s 641ms/step - loss: 0.3310 - accuracy: 0.8493 - val_loss: 0.3260 - val_accuracy: 0.8446\n",
      "Epoch 22/50\n",
      "45/45 [==============================] - 29s 652ms/step - loss: 0.3310 - accuracy: 0.8469 - val_loss: 0.3254 - val_accuracy: 0.8587\n",
      "Epoch 23/50\n",
      "45/45 [==============================] - 30s 658ms/step - loss: 0.3297 - accuracy: 0.8513 - val_loss: 0.3214 - val_accuracy: 0.8579\n",
      "Epoch 24/50\n",
      "45/45 [==============================] - 28s 630ms/step - loss: 0.3232 - accuracy: 0.8525 - val_loss: 0.3206 - val_accuracy: 0.8555\n",
      "Epoch 25/50\n",
      "45/45 [==============================] - 30s 662ms/step - loss: 0.3145 - accuracy: 0.8592 - val_loss: 0.3201 - val_accuracy: 0.8612\n",
      "Epoch 26/50\n",
      "45/45 [==============================] - 29s 634ms/step - loss: 0.3186 - accuracy: 0.8571 - val_loss: 0.3190 - val_accuracy: 0.8580\n",
      "Epoch 27/50\n",
      "45/45 [==============================] - 28s 633ms/step - loss: 0.3192 - accuracy: 0.8522 - val_loss: 0.3162 - val_accuracy: 0.8677\n",
      "Epoch 28/50\n",
      "45/45 [==============================] - 29s 649ms/step - loss: 0.3194 - accuracy: 0.8561 - val_loss: 0.3138 - val_accuracy: 0.8618\n",
      "Epoch 29/50\n",
      "45/45 [==============================] - 30s 664ms/step - loss: 0.3324 - accuracy: 0.8457 - val_loss: 0.3143 - val_accuracy: 0.8682\n",
      "Epoch 30/50\n",
      "45/45 [==============================] - 30s 665ms/step - loss: 0.3118 - accuracy: 0.8572 - val_loss: 0.3150 - val_accuracy: 0.8493\n",
      "Epoch 31/50\n",
      "45/45 [==============================] - 30s 666ms/step - loss: 0.3157 - accuracy: 0.8566 - val_loss: 0.3147 - val_accuracy: 0.8705\n",
      "Epoch 32/50\n",
      "45/45 [==============================] - 28s 629ms/step - loss: 0.3119 - accuracy: 0.8578 - val_loss: 0.3110 - val_accuracy: 0.8680\n",
      "Epoch 33/50\n",
      "45/45 [==============================] - 29s 635ms/step - loss: 0.2981 - accuracy: 0.8666 - val_loss: 0.3127 - val_accuracy: 0.8637\n",
      "Epoch 34/50\n",
      "45/45 [==============================] - 29s 645ms/step - loss: 0.3060 - accuracy: 0.8621 - val_loss: 0.3091 - val_accuracy: 0.8736\n",
      "Epoch 35/50\n",
      "45/45 [==============================] - 30s 665ms/step - loss: 0.3114 - accuracy: 0.8636 - val_loss: 0.3104 - val_accuracy: 0.8659\n",
      "Epoch 36/50\n",
      "45/45 [==============================] - 29s 654ms/step - loss: 0.2997 - accuracy: 0.8710 - val_loss: 0.3180 - val_accuracy: 0.8405\n",
      "Epoch 37/50\n",
      "45/45 [==============================] - 29s 641ms/step - loss: 0.3084 - accuracy: 0.8596 - val_loss: 0.3133 - val_accuracy: 0.8764\n",
      "Epoch 38/50\n",
      "45/45 [==============================] - 30s 664ms/step - loss: 0.2946 - accuracy: 0.8709 - val_loss: 0.3055 - val_accuracy: 0.8607\n",
      "Epoch 39/50\n",
      "45/45 [==============================] - 29s 641ms/step - loss: 0.2944 - accuracy: 0.8661 - val_loss: 0.3055 - val_accuracy: 0.8714\n",
      "Epoch 40/50\n",
      "45/45 [==============================] - 29s 654ms/step - loss: 0.2921 - accuracy: 0.8707 - val_loss: 0.3004 - val_accuracy: 0.8736\n",
      "Epoch 41/50\n",
      "45/45 [==============================] - 29s 644ms/step - loss: 0.2865 - accuracy: 0.8737 - val_loss: 0.3013 - val_accuracy: 0.8718\n",
      "Epoch 42/50\n",
      "45/45 [==============================] - 27s 604ms/step - loss: 0.2860 - accuracy: 0.8726 - val_loss: 0.3044 - val_accuracy: 0.8718\n",
      "Epoch 43/50\n",
      "45/45 [==============================] - 28s 630ms/step - loss: 0.2832 - accuracy: 0.8750 - val_loss: 0.3380 - val_accuracy: 0.8486\n",
      "Epoch 44/50\n",
      "45/45 [==============================] - 29s 654ms/step - loss: 0.3083 - accuracy: 0.8611 - val_loss: 0.3006 - val_accuracy: 0.8764\n",
      "Epoch 45/50\n",
      "45/45 [==============================] - 29s 636ms/step - loss: 0.2837 - accuracy: 0.8742 - val_loss: 0.3217 - val_accuracy: 0.8546\n",
      "Epoch 46/50\n",
      "45/45 [==============================] - 30s 656ms/step - loss: 0.2927 - accuracy: 0.8724 - val_loss: 0.2963 - val_accuracy: 0.8741\n",
      "Epoch 47/50\n",
      "45/45 [==============================] - 29s 641ms/step - loss: 0.2823 - accuracy: 0.8771 - val_loss: 0.3051 - val_accuracy: 0.8788\n",
      "Epoch 48/50\n",
      "45/45 [==============================] - 29s 637ms/step - loss: 0.2806 - accuracy: 0.8809 - val_loss: 0.2979 - val_accuracy: 0.8766\n",
      "Epoch 49/50\n",
      "45/45 [==============================] - 27s 609ms/step - loss: 0.2778 - accuracy: 0.8789 - val_loss: 0.2952 - val_accuracy: 0.8743\n",
      "Epoch 50/50\n",
      "45/45 [==============================] - 29s 641ms/step - loss: 0.2818 - accuracy: 0.8787 - val_loss: 0.2992 - val_accuracy: 0.8802\n"
     ]
    }
   ],
   "source": [
    "lstmUnits = 128\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x=inputs, y=labels, epochs=50, batch_size = 500, validation_split=0.2, shuffle=True, callbacks=[tensorboard_callback, cp_callback])\n",
    "model.save_weights(checkpoint_path.format(epoch=70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f1ec33664c4b9384\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f1ec33664c4b9384\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ x√°c ƒë·ªãnh ƒë·ªô ch√≠nh x√°c c·ªßa h·ªá th·ªëng, ta ƒë·∫øm s·ªë l∆∞·ª£ng labels kh·ªõp v·ªõi gi√° tr·ªã d·ª± ƒëo√°n (prediction). Sau ƒë√≥ t√≠nh ƒë·ªô ch√≠nh x√°c b·∫±ng c√°ch t√≠nh gi√° tr·ªã trung b√¨nh c·ªßa c√°c k·∫øt qu·∫£ tr·∫£ v·ªÅ ƒë√∫ng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S·ª≠ d·ª•ng Tensorboard ƒë·ªÉ visualize k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong qu√° tr√¨nh hu·∫•n luy·ªán, ch∆∞∆°ng tr√¨nh s·∫Ω ghi log v·ªÅ ƒë·ªô l·ªói v√† ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p train v√†o th∆∞ m·ª•c 'tensorboard', l∆∞u l·∫°i model sau m·ªói 2000 v√≤ng l·∫∑p ·ªü th∆∞ m·ª•c 'models'. Vi·ªác hu·∫•n luy·ªán tr√™n 30,000 v√≤ng l·∫∑p m·∫•t kho·∫£ng v√†i ti·∫øng v·ªõi GPU K80 ƒë∆∞·ª£c cung c·∫•p b·ªüi Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V·ªõi m·ªói v√≤ng l·∫∑p, ta s·∫Ω l·∫•y ra m·ªôt batch d·ªØ li·ªáu train ƒë·ªÉ ƒë∆∞a v√†o m·∫°ng s·ª≠ d·ª•ng `feed_dict`. v·ªõi c√°c tham s·ªë input v√† label l√† c√°c placeholders. B∆∞·ªõc hu·∫•n luy·ªán n√†y ƒë∆∞·ª£c l·∫∑p l·∫°i cho ƒë·∫øn khi h·∫øt s·ªë l·∫ßn c·∫ßn hu·∫•n luy·ªán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 23ms/step - loss: 0.2960 - accuracy: 0.8795\n",
      "Restored model, accuracy: 87.95%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_inputs, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "63/63 [==============================] - 3s 24ms/step - loss: 0.2548 - accuracy: 0.8840\n",
      "Restored model, accuracy: 87.95%\n"
     ]
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "loaded_model = twoLayerLSTM(lstmUnits, wordVectors).model()\n",
    "loaded_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "loaded_model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 180)\n",
      "(2000,)\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.2960 - accuracy: 0.8795\n",
      "Restored model, accuracy: 87.95%\n"
     ]
    }
   ],
   "source": [
    "print(test_inputs.shape)\n",
    "print(test_labels.shape)\n",
    "loss, acc = loaded_model.evaluate(test_inputs, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load m√¥ h√¨nh ƒë√£ train v√† ƒë√°nh gi√° m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th·ªùi gian hu·∫•n luy·ªán m·∫°ng kh√° l√¢u, n√™n trong qu√° tr√¨nh m·∫°ng ƒëang ƒë∆∞·ª£c hu·∫•n luy·ªán, ta s·∫Ω l∆∞u l·∫°i m·ªôt s·ªë checkpoint. ƒê·ªÉ c√≥ th·ªÉ test th·ª≠ tr√™n m·ªôt checkpoint m·ªõi nh·∫•t ta s·ª≠ d·ª•ng h√†m tf.train.latest_checkpoint v√† truy·ªÅn v√†o t√™n th∆∞ m·ª•c mu·ªën l·∫•y model m·ªõi nh·∫•t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau ƒë√≥, v·ªõi m·ªói batch d·ªØ li·ªáu test, ta s·∫Ω ti·∫øn h√†nh test v√† t√≠nh ƒë·ªô ch√≠nh x√°c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.6: Test m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do c√°c b·ªô test ƒë∆∞·ª£c l·∫•y ng·∫´u nhi√™n n√™n ƒë·ªô ch√≠nh x√°c trong qu√° tr√¨nh n√†y c≈©ng dao ƒë·ªông t·ª´ 70% ƒë·∫øn 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.7: Vi·∫øt h√†m t·ªïng h·ª£p ƒë·ªÉ d·ª± ƒëo√°n c·∫£m x√∫c t·ª´ c√¢u ti·∫øng Vi·ªát"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C√¢u cu·ªëi c√πng n√†y ƒë√≤i h·ªèi ƒë√≤i h·ªèi c√°c b·∫°n ph·∫£i v·∫≠n d·ª•ng t∆∞ duy t·ªïng h·ª£p ƒë·ªÉ gom t·∫•t c·∫£ nh·ªØng b∆∞·ªõc ƒë√£ th·ª±c hi·ªán tr∆∞·ªõc ƒë√≥ th√†nh m·ªôt quy tr√¨nh ho√†n ch·ªânh. C√°c b·∫°n c·∫ßn vi·∫øt m·ªôt h√†m ho√†n ch·ªânh v·ªõi ƒë·∫ßu v√†o l√†  m·ªôt c√¢u ti·∫øng Vi·ªát cho tr∆∞·ªõc, ƒë·∫ßu ra l√† cho bi·∫øt c√¢u tr√™n c√≥ c·∫£m x√∫c t√≠ch c·ª±c hay ti√™u c·ª±c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = 'M√≥n n√†y ƒÉn ngon m√™ ly lu√¥n. V·ªã ng·ªçt v√† th∆°m qu√° tr·ªùi qu√° ƒë·∫•t.'\n",
    "# TODO 3.7 C√°c b·∫°n v·∫≠n d·ª•ng to√†n b·ªô quy tr√¨nh ƒë√£ th·ª±c hi·ªán tr∆∞·ªõc ƒë√≥\n",
    "# ƒë·ªÉ d·ª± ƒëo√°n xem c√¢u n√†y c√≥ c·∫£m x√∫c t√≠ch c·ª±c hay ti√™u c·ª±c\n",
    "# C√¢u n√†y l√†m kh√° d√†i v√† c√≥ t√≠nh ch·∫•t t·ªïng h·ª£p\n",
    "input_sentence = cleanSentences(input_sentence)\n",
    "s = input_sentence.split()\n",
    "indexes = np.zeros((1, maxSeqLength))\n",
    "for i,w in enumerate(s):\n",
    "    indexes[0][i] = wordsList.index(w)\n",
    "\n",
    "base_input = loaded_model.layers[0].input\n",
    "base_output = loaded_model.layers[4].output\n",
    "output = tf.keras.layers.Dense(1,activation='softmax')(base_output)\n",
    "model_ = tf.keras.Model(base_input, output)\n",
    "pred = model_.predict(indexes)\n",
    "#because i change the basic indexing above\n",
    "if pred == 1:\n",
    "    print(input_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K·∫øt lu·∫≠n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nh∆∞ v·∫≠y qua b√†i t·∫≠p n√†y, c√°c b·∫°n ƒë∆∞·ª£c √¥n l·∫°i m√¥ h√¨nh Word2Vec v√† s·ª≠ d·ª•ng m√¥ h√¨nh n√†y ƒë·ªÉ bi·ªÉu di·ªÖn cho m·ªôt vƒÉn b·∫£n. S·ª≠ d·ª•ng c√°ch bi·ªÉu di·ªÖn n√†y ƒë·ªÉ ƒë∆∞a v√†o m√¥ h√¨nh RNN v·ªõi nhi·ªÅu ƒë∆°n v·ªã LSTM. C√°c b·∫°n c√≥ th·ªÉ th·ª≠ nghi·ªám tr√™n c√°c c·∫•u h√¨nh kh√°c nhau b·∫±ng c√°ch thay ƒë·ªïi c√°c hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
